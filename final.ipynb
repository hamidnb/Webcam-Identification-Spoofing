{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcee4367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import dlib\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance as dist\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\user\\Downloads\\iran\\FineTune.pt\"\n",
    "PERSON_PICTURE_DIR = r\"C:\\Users\\user\\Downloads\\iran\\Hamid\\Hamid\"\n",
    "\n",
    "FRAME_WIDTH = 640\n",
    "FRAME_HEIGHT = 480\n",
    "\n",
    "RECOGNITION_THRESHOLD = 0.6\n",
    "EAR_THRESHOLD = 0.25\n",
    "EAR_CONSEC_FRAMES = 3\n",
    "\n",
    "\n",
    "PREDICTOR_PATH = r\"C:\\Users\\user\\Downloads\\iran\\Face_Landmarks\"\n",
    "FACE_REC_MODEL_PATH = r\"C:\\Users\\user\\Downloads\\iran\\dlib_face_recognition_resnet_model_v1.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(PREDICTOR_PATH) #68\n",
    "facerec = dlib.face_recognition_model_v1(FACE_REC_MODEL_PATH)#128\n",
    "\n",
    "(L_START, L_END) = (42, 48)\n",
    "(R_START, R_END) = (36, 42)\n",
    "\n",
    "\n",
    "def calculate_ear(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])# measuring eye points \n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def load_reference_embeddings(person_dir): #create a uniqe ebmedding \n",
    "    reference_embeddings = {}\n",
    "    if detector is None or sp is None or facerec is None:\n",
    "        return reference_embeddings\n",
    "\n",
    "    person_path = Path(person_dir)\n",
    "    if not person_path.exists():\n",
    "        return reference_embeddings\n",
    "\n",
    "    person_name = person_path.name \n",
    "    person_embs = []\n",
    "    \n",
    "    image_files = list(person_path.glob(\"*.[jp][pn]g\")) + list(person_path.glob(\"*.jpeg\"))\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        img = cv.imread(str(img_path))\n",
    "        if img is None: continue\n",
    "\n",
    "        rgb_img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        faces = detector(rgb_img, 1)\n",
    "        \n",
    "        if len(faces) > 0:\n",
    "            landmarks = sp(rgb_img, faces[0])\n",
    "            embedding = np.array(facerec.compute_face_descriptor(rgb_img, landmarks))\n",
    "            person_embs.append(embedding)\n",
    "            \n",
    "    if person_embs:\n",
    "        reference_embeddings[person_name] = person_embs\n",
    "    \n",
    "    return reference_embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    cap = cv.VideoCapture(0)\n",
    "     \n",
    "    cap.set(cv.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "    cap.set(cv.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "    model = YOLO(MODEL_PATH)\n",
    "    reference_embeddings = load_reference_embeddings(PERSON_PICTURE_DIR)\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator(thickness=2)\n",
    "    label_annotator = sv.LabelAnnotator(text_scale=0.7, text_thickness=2, text_padding=4)\n",
    "    tracker = sv.ByteTrack()\n",
    "\n",
    "    liveness_state = {}\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Face Detection\n",
    "        # ________________________________________________________\n",
    "        result = model(frame, verbose=False, conf=0.5)[0]\n",
    "        detections = sv.Detections.from_ultralytics(result)\n",
    "        detections = tracker.update_with_detections(detections)\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            xyxy = detections.xyxy[i]\n",
    "            tracker_id = detections.tracker_id[i]\n",
    "\n",
    "            if tracker_id not in liveness_state:\n",
    "                liveness_state[tracker_id] = {\n",
    "                    \"blink_counter\": 0, \"total_blinks\": 0, \"name\": \"Unknown\", \n",
    "                    \"live\": False, \"is_identified\": False\n",
    "                }\n",
    "            state = liveness_state[tracker_id]\n",
    "\n",
    "            padding = 20\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            h, w, _ = frame.shape\n",
    "            x1_pad = max(0, x1 - padding)\n",
    "            y1_pad = max(0, y1 - padding)\n",
    "            x2_pad = min(w, x2 + padding)\n",
    "            y2_pad = min(h, y2 + padding)\n",
    "\n",
    "            dlib_rect = dlib.rectangle(x1_pad, y1_pad, x2_pad, y2_pad)\n",
    "            \n",
    "            try:\n",
    "                landmarks = sp(rgb_frame, dlib_rect)\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Liveness Detection\n",
    "                # ---------------------------------------\n",
    "                left_eye = np.array([[p.x, p.y] for p in landmarks.parts()[L_START:L_END]])\n",
    "                right_eye = np.array([[p.x, p.y] for p in landmarks.parts()[R_START:R_END]])\n",
    "                avg_ear = (calculate_ear(left_eye) + calculate_ear(right_eye)) / 2.0\n",
    "\n",
    "                if avg_ear < EAR_THRESHOLD:\n",
    "                    state[\"blink_counter\"] += 1\n",
    "                else:\n",
    "                    if state[\"blink_counter\"] >= EAR_CONSEC_FRAMES:\n",
    "                        state[\"total_blinks\"] += 1\n",
    "                        state[\"live\"] = True\n",
    "                    state[\"blink_counter\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Face Recognition\n",
    "                # ---------------------------------------\n",
    "                if not state[\"is_identified\"]:\n",
    "                    embedding = np.array(facerec.compute_face_descriptor(rgb_frame, landmarks))\n",
    "                    \n",
    "                    max_similarity = -1\n",
    "                    recognized_person = \"Unknown\"\n",
    "                    for ref_name, ref_embs_list in reference_embeddings.items():\n",
    "                        for ref_emb in ref_embs_list:\n",
    "                            similarity = cosine_similarity(embedding.reshape(1, -1), ref_emb.reshape(1, -1))[0][0]\n",
    "                            if similarity > max_similarity:\n",
    "                                max_similarity = similarity\n",
    "                                recognized_person = ref_name\n",
    "                    \n",
    "                    if max_similarity > RECOGNITION_THRESHOLD:\n",
    "                        state[\"name\"] = recognized_person\n",
    "                    \n",
    "                    state[\"is_identified\"] = True\n",
    "            \n",
    "            except Exception as e:\n",
    "                state[\"name\"] = \"Face Not Aligned\"\n",
    "\n",
    "            liveness_status = \"Live\" if state[\"live\"] else \"Spoof/Photo\"\n",
    "            label = f\"{state['name']} | {liveness_status} (Blinks: {state['total_blinks']})\"\n",
    "            labels.append(label)\n",
    "\n",
    "        current_tracker_ids = set(detections.tracker_id)\n",
    "        for tracker_id in list(liveness_state.keys()):\n",
    "            if tracker_id not in current_tracker_ids:\n",
    "                del liveness_state[tracker_id]\n",
    "\n",
    "        frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "        frame = label_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "\n",
    "        cv.imshow(\"Face Recognition & Liveness\", frame)\n",
    "\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf1495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd050a2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7490ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76aca024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No face detected in reference image C:\\Users\\user\\Downloads\\iran\\Hamid\\Hamid\\photo_1_2025-05-29_11-22-57.jpg. Skipping.\n",
      "Warning: No face detected in reference image C:\\Users\\user\\Downloads\\iran\\Hamid\\Hamid\\photo_2_2025-05-29_11-22-57.jpg. Skipping.\n",
      "Warning: No face detected in reference image C:\\Users\\user\\Downloads\\iran\\Hamid\\Hamid\\photo_3_2025-05-29_11-22-57.jpg. Skipping.\n",
      "Warning: No face detected in reference image C:\\Users\\user\\Downloads\\iran\\Hamid\\Hamid\\photo_4_2025-05-29_11-22-57.jpg. Skipping.\n",
      "Warning: No face detected in reference image C:\\Users\\user\\Downloads\\iran\\Hamid\\Hamid\\photo_5_2025-05-29_11-22-57.jpg. Skipping.\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import dlib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\user\\Downloads\\iran\\FineTune.pt\"\n",
    "PERSON_PICTURE_DIR = r\"C:\\Users\\user\\Downloads\\iran\\Hamid\\Hamid\"\n",
    "\n",
    "FRAME_WIDTH = 640\n",
    "FRAME_HEIGHT = 480\n",
    "\n",
    "RECOGNITION_THRESHOLD = 0.6\n",
    "EAR_THRESHOLD = 0.25\n",
    "EAR_CONSEC_FRAMES = 3\n",
    "\n",
    "# Head Pose Estimation parameters (new)\n",
    "MODEL_POINTS = np.array([\n",
    "    (0.0, 0.0, 0.0),             # Nose tip\n",
    "    (0.0, -330.0, -65.0),        # Chin\n",
    "    (-225.0, 170.0, -135.0),     # Left eye left corner\n",
    "    (225.0, 170.0, -135.0),      # Right eye right corner\n",
    "    (-150.0, -150.0, -125.0),    # Left mouth corner\n",
    "    (150.0, -150.0, -125.0)      # Right mouth corner\n",
    "], dtype=\"double\")\n",
    "\n",
    "PREDICTOR_PATH = r\"C:\\Users\\user\\Downloads\\iran\\Face_Landmarks\"\n",
    "FACE_REC_MODEL_PATH = r\"C:\\Users\\user\\Downloads\\iran\\dlib_face_recognition_resnet_model_v1.dat\"\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "sp = dlib.shape_predictor(PREDICTOR_PATH)\n",
    "facerec = dlib.face_recognition_model_v1(FACE_REC_MODEL_PATH)\n",
    "\n",
    "(L_START, L_END) = (42, 48)\n",
    "(R_START, R_END) = (36, 42)\n",
    "\n",
    "\n",
    "def calculate_ear(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "def load_reference_embeddings(person_dir):\n",
    "    reference_embeddings = {}\n",
    "    \n",
    "    # Re-adding essential safety checks for model loading and directory existence\n",
    "    if detector is None or sp is None or facerec is None:\n",
    "        print(\"Error: Dlib models not loaded. Cannot load reference embeddings.\")\n",
    "        return reference_embeddings\n",
    "    \n",
    "    person_path = Path(person_dir)\n",
    "    if not person_path.exists():\n",
    "        print(f\"Error: Reference person directory not found: {person_dir}\")\n",
    "        return reference_embeddings\n",
    "    \n",
    "    person_name = person_path.name \n",
    "    person_embs = []\n",
    "    \n",
    "    image_files = list(person_path.glob(\"*.[jp][pn]g\")) + list(person_path.glob(\"*.jpeg\"))\n",
    "    \n",
    "    for img_path in image_files:\n",
    "        img = cv.imread(str(img_path))\n",
    "        if img is None: # This check is also essential for image loading\n",
    "            print(f\"Warning: Could not load image {img_path}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        rgb_img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "        faces = detector(rgb_img, 1)\n",
    "        \n",
    "        # ESSENTIAL FIX: Re-adding the check for detected faces\n",
    "        if len(faces) > 0:\n",
    "            landmarks = sp(rgb_img, faces[0]) \n",
    "            embedding = np.array(facerec.compute_face_descriptor(rgb_img, landmarks))\n",
    "            person_embs.append(embedding)\n",
    "        else:\n",
    "            print(f\"Warning: No face detected in reference image {img_path}. Skipping.\")\n",
    "            \n",
    "    if person_embs:\n",
    "        reference_embeddings[person_name] = person_embs\n",
    "    \n",
    "    return reference_embeddings\n",
    "\n",
    "def main():\n",
    "    # Re-adding essential safety checks for main function setup\n",
    "    if sp is None or facerec is None:\n",
    "        print(\"Error: Dlib shape predictor or face recognition model not available. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    cap = cv.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam. Exiting.\")\n",
    "        return\n",
    "        \n",
    "    cap.set(cv.CAP_PROP_FRAME_WIDTH, FRAME_WIDTH)\n",
    "    cap.set(cv.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "    # Initialize camera matrix and distortion coefficients for head pose\n",
    "    focal_length = FRAME_WIDTH\n",
    "    center = (FRAME_WIDTH/2, FRAME_HEIGHT/2)\n",
    "    camera_matrix = np.array(\n",
    "        [[focal_length, 0, center[0]],\n",
    "         [0, focal_length, center[1]],\n",
    "         [0, 0, 1]], dtype = \"double\"\n",
    "    )\n",
    "    dist_coeffs = np.zeros((4,1)) # Assuming no lens distortion\n",
    "\n",
    "    model = YOLO(MODEL_PATH)\n",
    "    reference_embeddings = load_reference_embeddings(PERSON_PICTURE_DIR)\n",
    "\n",
    "    box_annotator = sv.BoxAnnotator(thickness=2)\n",
    "    label_annotator = sv.LabelAnnotator(text_scale=0.7, text_thickness=2, text_padding=4)\n",
    "    tracker = sv.ByteTrack()\n",
    "\n",
    "    liveness_state = {} # Stores blink count, recognition status, and now head pose state\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: \n",
    "            print(\"Error: Could not read frame from webcam. Exiting loop.\")\n",
    "            break\n",
    "\n",
    "        rgb_frame = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Face Detection\n",
    "        # ________________________________________________________\n",
    "        result = model(frame, verbose=False, conf=0.5)[0]\n",
    "        detections = sv.Detections.from_ultralytics(result)\n",
    "        detections = tracker.update_with_detections(detections)\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(detections)):\n",
    "            xyxy = detections.xyxy[i]\n",
    "            tracker_id = detections.tracker_id[i]\n",
    "\n",
    "            if tracker_id not in liveness_state:\n",
    "                liveness_state[tracker_id] = {\n",
    "                    \"blink_counter\": 0, \n",
    "                    \"total_blinks\": 0, \n",
    "                    \"name\": \"Unknown\", \n",
    "                    \"live\": False, \n",
    "                    \"is_identified\": False,\n",
    "                    \"head_pose_moved\": False, # New for head pose\n",
    "                    \"head_pose_count\": 0      # New for head pose\n",
    "                }\n",
    "            state = liveness_state[tracker_id]\n",
    "\n",
    "            padding = 20\n",
    "            x1, y1, x2, y2 = map(int, xyxy)\n",
    "            h, w, _ = frame.shape\n",
    "            x1_pad = max(0, x1 - padding)\n",
    "            y1_pad = max(0, y1 - padding)\n",
    "            x2_pad = min(w, x2 + padding)\n",
    "            y2_pad = min(h, y2 + padding)\n",
    "\n",
    "            dlib_rect = dlib.rectangle(x1_pad, y1_pad, x2_pad, y2_pad)\n",
    "            \n",
    "            # Re-adding a try-except block here is highly recommended for live processing\n",
    "            try: \n",
    "                landmarks = sp(rgb_frame, dlib_rect)\n",
    "                \n",
    "                # Liveness Detection (Blink & Head Pose)\n",
    "                # ---------------------------------------\n",
    "                left_eye = np.array([[p.x, p.y] for p in landmarks.parts()[L_START:L_END]])\n",
    "                right_eye = np.array([[p.x, p.y] for p in landmarks.parts()[R_START:R_END]])\n",
    "                avg_ear = (calculate_ear(left_eye) + calculate_ear(right_eye)) / 2.0\n",
    "\n",
    "                # Blink Detection\n",
    "                if avg_ear < EAR_THRESHOLD:\n",
    "                    state[\"blink_counter\"] += 1\n",
    "                else:\n",
    "                    if state[\"blink_counter\"] >= EAR_CONSEC_FRAMES:\n",
    "                        state[\"total_blinks\"] += 1\n",
    "                        # A blink is a strong sign of liveness\n",
    "                        state[\"live\"] = True \n",
    "                    state[\"blink_counter\"] = 0\n",
    "\n",
    "                # Head Pose Estimation (New Liveness Check)\n",
    "                image_points = np.array([\n",
    "                    (landmarks.parts()[30].x, landmarks.parts()[30].y),    # Nose tip\n",
    "                    (landmarks.parts()[8].x, landmarks.parts()[8].y),      # Chin\n",
    "                    (landmarks.parts()[36].x, landmarks.parts()[36].y),    # Left eye left corner\n",
    "                    (landmarks.parts()[45].x, landmarks.parts()[45].y),    # Right eye right corner\n",
    "                    (landmarks.parts()[48].x, landmarks.parts()[48].y),    # Left mouth corner\n",
    "                    (landmarks.parts()[54].x, landmarks.parts()[54].y)     # Right mouth corner\n",
    "                ], dtype=\"double\")\n",
    "\n",
    "                (success, rotation_vector, translation_vector) = cv.solvePnP(\n",
    "                    MODEL_POINTS, image_points, camera_matrix, dist_coeffs, flags=cv.SOLVEPNP_ITERATIVE\n",
    "                )\n",
    "\n",
    "                rmat, jac = cv.Rodrigues(rotation_vector)\n",
    "                angles, mtxR, mtxQ, Qx, Qy, Qz = cv.RQDecomp3x3(rmat)\n",
    "                \n",
    "                current_yaw = angles[1] \n",
    "\n",
    "                if not state[\"head_pose_moved\"]:\n",
    "                    if abs(current_yaw) > 10: \n",
    "                        state[\"head_pose_moved\"] = True\n",
    "                        state[\"head_pose_count\"] += 1 \n",
    "                        state[\"live\"] = True \n",
    "                \n",
    "                # Face Recognition\n",
    "                # ---------------------------------------\n",
    "                if not state[\"is_identified\"]:\n",
    "                    embedding = np.array(facerec.compute_face_descriptor(rgb_frame, landmarks))\n",
    "                    \n",
    "                    max_similarity = -1\n",
    "                    recognized_person = \"Unknown\"\n",
    "                    for ref_name, ref_embs_list in reference_embeddings.items():\n",
    "                        for ref_emb in ref_embs_list:\n",
    "                            similarity = cosine_similarity(embedding.reshape(1, -1), ref_emb.reshape(1, -1))[0][0]\n",
    "                            if similarity > max_similarity:\n",
    "                                max_similarity = similarity\n",
    "                                recognized_person = ref_name\n",
    "                    \n",
    "                    if max_similarity > RECOGNITION_THRESHOLD:\n",
    "                        state[\"name\"] = recognized_person\n",
    "                    \n",
    "                    state[\"is_identified\"] = True\n",
    "            \n",
    "            except Exception as e:\n",
    "                # If dlib processing fails (e.g., landmarks not found or solvePnP fails)\n",
    "                state[\"name\"] = \"Face Not Aligned/Processed\"\n",
    "                # print(f\"Processing error for tracker_id {tracker_id}: {e}\") # Debugging aid\n",
    "\n",
    "            liveness_status = \"Live\" if state[\"live\"] else \"Spoof/Photo\"\n",
    "            label = f\"{state['name']} | {liveness_status} (Blinks: {state['total_blinks']}) (Head: {state['head_pose_count']})\" \n",
    "            labels.append(label)\n",
    "\n",
    "        current_tracker_ids = set(detections.tracker_id)\n",
    "        for tracker_id in list(liveness_state.keys()):\n",
    "            if tracker_id not in current_tracker_ids:\n",
    "                del liveness_state[tracker_id]\n",
    "\n",
    "        frame = box_annotator.annotate(scene=frame, detections=detections)\n",
    "        frame = label_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "\n",
    "        cv.imshow(\"Face Recognition & Liveness\", frame)\n",
    "\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acedd5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
